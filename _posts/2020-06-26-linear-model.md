---
layout: post
title:  "ML 笔记：线性模型"
date:   2020-06-26 09:00:00 +0800
categories: machine-learning 
---

目录
- [1 基本形式](#基本形式)
- [2 线性回归](#线性回归)
- [3 对数几率回归](#对数几率回归)
- [4 线性判别分析](#线性判别分析)
- [5 多分类学习](#多分类学习)
- [6 类别不平衡问题](#类别不平衡问题)


## 基本形式

给定由 d 个属性描述的示例 $$\bm x = (x_1, \dots, x_d)$$，*线性模型（linear model）* 试图学得一个通过属性的线性组合来进行预测的函数，即

$$ f(\bm x) = w_1 x_1 + \dots + w_d x_d + b $$

一般用向量形式写成

$$ f(\bm x) = \bm w^T \bm x + b $$

线性模型形式简单、易于建模，但却蕴含着机器学习中一些重要的基本思想。许多功能更为强大的非线性模型（nonlinear model）可在线性模型的基础上引入层级结构或高维映射而得。此外，由于 $$\bm w$$ 直观表达了各属性在预测中的重要性，因此线性模型有很好的 *可解释性（comprehensibility）* 。

本章介绍几种经典的线性模型。我们先从回归任务开始，然后讨论二分类和多分类任务。

## 线性回归

*线性回归（linear regression）* 试图学得一个线性模型以尽可能准确地预测实值输出标记，即

$$ f(\bm x_i) = \bm w^T x_i + b $$ ，使得 $$ f(\bm x_i) \sim y_i $$

回归任务中最常用的性能度量是均方误差。为便于讨论，把 $$ \bm w $$ 和 b 吸收入向量形式 $$ \hat \bm w = (\bm w; b) $$，把数据集 D 表示为一个 $$ m \times (d + 1) $$ 大小的矩阵 $$ \bm X $$，其中每行对应于一个示例，行中前 d 个元素对应于示例的 d 个属性值，最后一个元素恒置为 1，则有

$$ \hat \bm w^* = \arg \min_{\hat \bm w} {(\bm y - \bm X \hat \bm w)}^T (\bm y - \bm X \hat w) $$

均方误差的几何意义是 *欧氏距离（Euclidean distance）* 。基于均方误差最小化来进行模型求解的方法称为 *最小二乘法（least square method）*。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

求解 w 和 b 使 $$E_{\hat w} = {(y-X \hat w)}^T(y - Xw)$$ 最小化的过程，称为线性回归模型的最小二乘 *参数估计（parameter estimation）*。令

$$ \frac {\partial E_{\hat w}} {\partial \hat w} = 2 X^T(X \hat w - y) = 0 $$

可得 $$ \hat w $$ 最优解的 *闭式（closed-form）解*。由于涉及矩阵逆的计算，下面做一个简单讨论。

## 对数几率回归

## 线性判别分析

## 多分类学习

## 类别不平衡问题
